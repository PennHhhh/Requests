'''
    爬虫：通过编写程序模拟浏览器上网，然后让其去互联网上抓取数据的过程

    爬虫在场景中的分类:
        通用爬虫：抓取系统重要组成部分，抓取的是一整张页面数据
        聚焦爬虫：建立在通用爬虫的基础上，抓取的是页面中特定的局部内容
        增量式爬虫：检测网站中数据更新的情况，只会抓取网站中最新更新出来的数据

    反爬机制：门户网站可以通过制定相应的策略夥技术手段，防止爬虫程序进行网站数据的爬取
        robots.txt协议：规定哪些可爬，哪些不行

    反反爬策略：爬虫程序可以通过制定相关的策略或技术手段，破解门户网站中具备的反爬机制

    http协议：服务器和客户端进行数据交互的一种形式
        常用的请求头信息：
            User-Agent:请求载体的身份标识
            Connection:请求完毕后，断开连接还是保持连接
        常用响应头信息：
            Connection:服务器响应回客户端的数据类型

    https协议：安全的超文本传输协议

    加密方式：
        对称密钥加密：将锁和密钥一起传过去（弊端：被拦截钥匙和锁都被拦截了，直接解密）
        非对称密钥加密：将锁和公钥一起传过去，私钥另外传，只有公钥和私钥都有才能开锁（弊端：公钥是可以被篡改）
        证书密钥加密：找第三方证书认证机构，给公钥做防伪
'''

'''
requests模块：python中原生的一款基于网络请求的模块，功能强大，简单便捷，效率高。作用：模拟浏览器发请求
使用：（requests模块的编码流程）
    - 指定url
    - UA伪装
    - 发起请求
    - 获取响应数据
    - 持久化存储

'''

import requests
if __name__ == '__main__':
    # 1.指定url
    url = 'https://www.sogou.com/'
    # 2.发起请求
    # get方法会返回一个响应对象
    response = requests.get(url=url)
    # 3.获取响应数据，text返回的是字符串形式的响应数据
    page_text = response.text
    print(page_text)
    # 4.持久化存储
    with open('./sogou.html', 'w', encoding='utf-8') as fp:
        fp.write(page_text)
    print('爬取数据结束')
